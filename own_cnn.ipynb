{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b03186f",
   "metadata": {},
   "source": [
    "## Predicting Remaining Useful Life (RUL) of an Engine using a CNN\n",
    "\n",
    "In diesem Notebook wird ein **CNN** zu großen Teilen **selbst implementiert**, indem dem die **Loss-Function (MSE)** selbst geschrieben wurde, aber vor allem in PyTorch der **Forward- und Backward-Pass** folgender Layer selbst überschrieben wurde: Convolutional-Layer, LeakyReLU Activation Function, Dense-Layer, Flatten-Layer. Dies erfolgte auf eigenen Recherchen zu den **mathematischen Herleitungen** und Hintergründen der einzelnen Layer, welche ebenfalls in Kurzform dargelegt und gezeigt werden.\n",
    "\n",
    "Genaurere Implementationsdetails finden sich ausführlich erklärt in der `README.md`, unsere **Dokumentation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1db5b4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b12c8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import (\n",
    "    FILENAME,\n",
    "    CHECKPOINT_PATH,\n",
    "    LEARNING_RATE,\n",
    "    BATCH_SIZE,\n",
    "    WIN_LEN,\n",
    "    LOAD_MODEL,\n",
    ")\n",
    "\n",
    "# device konfiguration setzen für optimiertes Training\n",
    "DEVICE: torch.device = torch.device(\n",
    "    \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ") # mps für mac"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f7509b",
   "metadata": {},
   "source": [
    "## Layer-Definitionen\n",
    "\n",
    "Im Folgenden werden die einzelnen Layer des Neuronalen Netzes selbst implementiert und mathematisch hergeleitet, entsprechend der eigenen Recherchen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0ff88d",
   "metadata": {},
   "source": [
    "## Custom 2D Convolution Layer (`CustomConv2d`)\n",
    "\n",
    "Eine 2D-Convolution lässt eine Menge lernbarer Kernel über eine Eingabe gleiten und\n",
    "berechnet an jeder Position ein Skalarprodukt. Das Ergebnis ist eine neue Menge an\n",
    "Feature Maps, wobei jede ein anderes lokales Muster erkennt wie z.B. charakteristische\n",
    "Verläufe in den Sensordaten.\n",
    "\n",
    "Anstatt PyTorchs `nn.Conv2d` zu verwenden, wird hier `torch.autograd.Function` genutzt,\n",
    "um sowohl Forward- als auch Backward-Pass explizit zu implementieren. Das macht jeden\n",
    "Schritt der Gradientenberechnung sichtbar und nachvollziehbar.\n",
    "\n",
    "---\n",
    "\n",
    "### Forward Pass\n",
    "\n",
    "Die Eingabe hat die Form\n",
    "\n",
    "$$X \\in \\mathbb{R}^{B \\times C_{\\text{in}} \\times H \\times W}$$\n",
    "\n",
    "mit $B$ Samples im Batch, $C_{\\text{in}}$ Input-Channels und räumlicher Größe $H \\times\n",
    "W$. Der Layer hat $C_{\\text{out}}$ Kernel der Form $C_{\\text{in}} \\times K_h \\times\n",
    "K_w$, sowie einen Bias $b \\in \\mathbb{R}^{C_{\\text{out}}}$.\n",
    "\n",
    "---\n",
    "\n",
    "#### Schritt 1 — `im2col`\n",
    "\n",
    "Anstatt das Sliding Window direkt zu implementieren, wird die Eingabe zunächst mit `F.unfold` in eine Spaltenmatrix umgeformt:\n",
    "\n",
    "$$X_{\\text{col}} \\in \\mathbb{R}^{B \\times (C_{\\text{in}} \\cdot K_h \\cdot K_w) \\times L},\n",
    "\\quad L = H_{\\text{out}} \\cdot W_{\\text{out}}$$\n",
    "\n",
    "Jede Spalte in $X_{\\text{col}}$ ist ein geflachter (flatten) Ausschnitt der Eingabe, auf\n",
    "dem der Kernel an dieser Position liegt.\n",
    "\n",
    "---\n",
    "\n",
    "#### Schritt 2 — Gewichte flatten\n",
    "\n",
    "Die Kernel werden in eine 2D-Matrix umgeformt, sodass die Cross-Correlation als\n",
    "Matrixmultiplikation berechnet werden kann:\n",
    "\n",
    "$$W_{\\text{flat}} \\in \\mathbb{R}^{C_{\\text{out}} \\times (C_{\\text{in}} \\cdot K_h \\cdot K_w)}$$\n",
    "\n",
    "Hierbei ist jede Zeile eine Feature Map.\n",
    "\n",
    "---\n",
    "\n",
    "#### Schritt 3 — Matrixmultiplikation\n",
    "\n",
    "$$Y_{\\text{flat}} = W_{\\text{flat}} \\cdot X_{\\text{col}} + b$$\n",
    "\n",
    "Der Bias wird über alle $L$ Positionen gebroadcastet. Das Ergebnis wird umgeformt zu\n",
    "\n",
    "$$Y \\in \\mathbb{R}^{B \\times C_{\\text{out}} \\times H_{\\text{out}} \\times W_{\\text{out}}}$$\n",
    "\n",
    "wobei die räumliche Ausgabegröße aus Stride $s$ und Padding $p$ folgt:\n",
    "\n",
    "$$H_{\\text{out}} = \\frac{H + 2p - K_h}{s} + 1, \\qquad W_{\\text{out}} = \\frac{W + 2p - K_w}{s} + 1$$\n",
    "\n",
    "---\n",
    "\n",
    "### Backward Pass\n",
    "\n",
    "Gegeben sei $\\frac{\\partial \\mathcal{L}}{\\partial Y} \\in \\mathbb{R}^{B \\times C_{\\text{out}} \\times H_{\\text{out}} \\times W_{\\text{out}}}$ aus der nachfolgenden Schicht.\n",
    "Daraus können drei Gradienten berechnet werden:\n",
    "\n",
    "#### Gradient der Gewichte\n",
    "\n",
    "Der eingehende Gradienten auf $(B, C_{\\text{out}}, L)$ muss umgeformt und über den Batch\n",
    "aufsummieren werden:\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial W} = \\sum_{b=1}^{B} \\frac{\\partial \\mathcal{L}}{\\partial Y_{\\text{flat}}}[b] \\cdot X_{\\text{col}}[b]^\\top \\;\\in \\mathbb{R}^{C_{\\text{out}} \\times C_{\\text{in}} \\times K_h \\times K_w}$$\n",
    "\n",
    "#### Gradient des Bias\n",
    "\n",
    "Entspricht der Summe über Batch- und räumliche Dimensionen:\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial b} = \\sum_{b,i,j} \\frac{\\partial \\mathcal{L}}{\\partial Y_{b,:,i,j}} \\;\\in \\mathbb{R}^{C_{\\text{out}}}$$\n",
    "\n",
    "#### Gradient des Inputs\n",
    "\n",
    "Die transponierte Gewichtsmatrix wird mit dem geflachten (flatten) Gradienten\n",
    "multipliziert und anschließend mit `F.fold` (die Umkehroperation von `F.unfold`) wieder\n",
    "in die ursprüngliche Eingabeform zurück transformiert:\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial X_{\\text{col}}} = W_{\\text{flat}}^\\top \\cdot \\frac{\\partial \\mathcal{L}}{\\partial Y_{\\text{flat}}} \\;\\xrightarrow{\\text{fold}}\\; \\frac{\\partial \\mathcal{L}}{\\partial X} \\in \\mathbb{R}^{B \\times C_{\\text{in}} \\times H \\times W}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eea91108",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2dFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(\n",
    "        ctx, x: Tensor, weight: Tensor, \n",
    "        bias: Tensor, stride: int, padding: int\n",
    "    ) -> Tensor:\n",
    "\n",
    "        B, C_in, H, W = x.shape\n",
    "        C_out, _, K_h, K_w = weight.shape\n",
    "        # C_out: Anzahl der Filter + Tiefe des Outputs\n",
    "        # C_in: Tiefe des Inputs + Tiefe des Filters\n",
    "\n",
    "        # im2col methode: matrix multiplikation für convolution\n",
    "        # \"ausfalten\" der input matrix in spalten für jedes sliding window\n",
    "        x_col: Tensor = F.unfold(\n",
    "            x, kernel_size=(K_h, K_w), padding=padding, stride=stride\n",
    "        )\n",
    "        # x_col: (B, C_in*K_h*K_w, L)\n",
    "        # L = H_out * W_out\n",
    "\n",
    "        # Gewichte \"flachen\" (flatten) für matrix multiplikation\n",
    "        # / richtige Form\n",
    "        weight_flat: Tensor = weight.view(C_out, C_in * K_h * K_w)\n",
    "        # (C_out, C_in*K_h*K_w)\n",
    "\n",
    "        # Matrix multiplikation\n",
    "        out: Tensor = weight_flat @ x_col\n",
    "        # out: (C_out, C_in*K_h*K_w) @ (C_in*K_h*K_w, L) -> (C_out, L)\n",
    "        # mit der Batch Dimension: (B, C_out, L)\n",
    "\n",
    "        if bias is not None:\n",
    "            out: Tensor = out + bias.view(1, C_out, 1)\n",
    "\n",
    "        # Shape des Outputs berechnen\n",
    "        H_out: int = (H + 2 * padding - K_h) // stride + 1\n",
    "        W_out: int = (W + 2 * padding - K_w) // stride + 1\n",
    "\n",
    "        out: Tensor = out.view(B, C_out, H_out, W_out)\n",
    "\n",
    "        # Speichern für den Backward-Pass\n",
    "        ctx.save_for_backward(x, weight, bias, x_col)\n",
    "        ctx.stride = stride\n",
    "        ctx.padding = padding\n",
    "        ctx.kernel_size = (K_h, K_w)\n",
    "\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(\n",
    "        ctx, grad_output: Tensor\n",
    "    ) -> tuple[Tensor, Tensor, Tensor, None, None]:\n",
    "\n",
    "        x, weight, bias, x_col = ctx.saved_tensors\n",
    "        stride: int = ctx.stride\n",
    "        padding: int = ctx.padding\n",
    "        K_h, K_w = ctx.kernel_size\n",
    "\n",
    "        B, C_out, _, _ = grad_output.shape\n",
    "        C_out, C_in, _, _ = weight.shape\n",
    "\n",
    "        grad_output_flat: Tensor = grad_output.view(B, C_out, -1)\n",
    "        # (B, C_out, L) mit L: H_out x W_out\n",
    "\n",
    "        weight_flat: Tensor = weight.view(C_out, -1)\n",
    "\n",
    "        # Gradient der Gewichte berechnen\n",
    "        grad_weight: Tensor = torch.zeros_like(weight)\n",
    "        for b in range(B):\n",
    "            grad_weight += (\n",
    "                grad_output_flat[b] @ x_col[b].T\n",
    "            ).view_as(weight)\n",
    "\n",
    "        # Gradient des Bias berechnen\n",
    "        grad_bias = None\n",
    "        if bias is not None:\n",
    "            grad_bias: Tensor = grad_output.sum(dim=(0, 2, 3))\n",
    "        # summieren über Batch, H_out und W_out Dimensionen -> (C_out,)\n",
    "\n",
    "        # Gradient des Inputs berechnen\n",
    "        grad_input_col: Tensor = weight_flat.T @ grad_output_flat\n",
    "        # (B, C_in*K*K, L)\n",
    "\n",
    "        # zurücktransformieren in die ursprüngliche Input-Form\n",
    "        grad_input: Tensor = F.fold(\n",
    "            grad_input_col,\n",
    "            output_size=(x.shape[2], x.shape[3]),\n",
    "            kernel_size=(K_h, K_w),\n",
    "            padding=padding,\n",
    "            stride=stride,\n",
    "        )\n",
    "        # grad_input: (B, C_in, H, W)\n",
    "\n",
    "        return grad_input, grad_weight, grad_bias, None, None\n",
    "\n",
    "\n",
    "class CustomConv2d(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: int,\n",
    "        stride: int = 1,\n",
    "        padding: int = 1,\n",
    "        bias: bool = True,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        k_h = k_w = kernel_size\n",
    "        # random Parameterinitialisierung mit der He-Initialisierung\n",
    "        # mit der Dimension: out_channels x in_channels x k_h x k_w\n",
    "        # 64 Feature Maps mit je 64 Kernels à 3x3\n",
    "        scale: float = (2 / (in_channels * k_h * k_w)) ** 0.5\n",
    "        self.weight: nn.Parameter = nn.Parameter(\n",
    "            torch.randn(out_channels, in_channels, k_h, k_w) * scale\n",
    "        )\n",
    "\n",
    "        #Vektor mit Länge out_channels, da jeder Kernel einen Bias hat\n",
    "        self.bias: nn.Parameter = (\n",
    "            nn.Parameter(torch.zeros(out_channels)) if bias else None\n",
    "        )\n",
    "\n",
    "        self.stride: int = stride\n",
    "        self.padding: int = padding\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return Conv2dFunction.apply(\n",
    "            x, self.weight, self.bias, self.stride, self.padding\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bb34ae",
   "metadata": {},
   "source": [
    "#### Activation Function: Leaky ReLU\n",
    "\n",
    "Aktivierungsfunktionen führen die Nicht-Linearität ein, die neuronale Netze brauchen um komplexe Muster zu lernen. Ohne sie würde das gesamte Netz, egal wie viele Schichten, auf eine einzige lineare Abbildung Y = WX + b reduziert werden. Sie werden elementweise auf den Output jedes Neurons angewendet.\n",
    "\n",
    "- Definition: $$f(x)=\n",
    "\\begin{cases}\n",
    "x & x > 0, \\\\\n",
    "\\alpha x & x \\leq 0,\n",
    "\\end{cases}$$\n",
    "- Ableitung: $$\\frac{\\partial f}{\\partial x} =\n",
    "\\begin{cases}\n",
    "1 & x > 0, \\\\\n",
    "\\alpha & x \\leq 0.\n",
    "\\end{cases}$$\n",
    "\n",
    "Leaky ReLU gibt negativen Werten einen kleinen, aber nicht-null Gradienten (α typischerweise zwischen 1e-3 und 0.1, hier: 0.01). Das verhindert das sogenannte \"Dying ReLU\"-Problem, bei dem Neuronen dauerhaft inaktiv werden und nichts mehr zum Lernprozess beitragen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77252afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeakyReLUFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x: Tensor, alpha: float) -> Tensor:\n",
    "        ctx.alpha = alpha\n",
    "        ctx.save_for_backward(x)\n",
    "        return torch.where(x > 0, x, alpha * x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output: Tensor) -> tuple[Tensor, None]:\n",
    "        (x,) = ctx.saved_tensors\n",
    "        alpha = ctx.alpha\n",
    "        grad_input = torch.where(\n",
    "            x > 0, grad_output, grad_output * alpha\n",
    "        )\n",
    "        return grad_input, None\n",
    "\n",
    "\n",
    "class CustomLeakyReLU(nn.Module):\n",
    "    def __init__(self, alpha: float = 0.01) -> None:\n",
    "        super().__init__()\n",
    "        if alpha < 0:\n",
    "            raise ValueError(\"alpha must be non-negative\")\n",
    "        self.alpha: float = alpha\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return LeakyReLUFunction.apply(x, self.alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765b6db7",
   "metadata": {},
   "source": [
    "### Flatten Layer\n",
    "\n",
    "Nach dem Adaptive Average Pooling hat jeder Channel nur noch einen einzigen Skalar. Der Tensor hat also die Form `(B, 64, 1, 1)`. Der Flatten Layer klappt die beiden räumlichen Dimensionen `1x1` zusammen, sodass ein Vektor der Form `(B, 64)` entsteht, ein Wert pro Channel pro Sample. Genau das erwartet der nachfolgende `CustomDense(64, 64)` als Eingabe.\n",
    "\n",
    "Der Flatten Layer hat keine lernbaren Parameter, er ist eine reine Umformung.\n",
    "\n",
    "---\n",
    "\n",
    "#### Forward\n",
    "\n",
    "$$\n",
    "X \\in \\mathbb{R}^{B \\times 64 \\times 1 \\times 1}\n",
    "\\;\\xrightarrow{\\text{reshape}}\\;\n",
    "Y \\in \\mathbb{R}^{B \\times 64}\n",
    "$$\n",
    "\n",
    "Da nach dem Pooling $H = W = 1$ gilt, ergibt sich $C \\cdot H \\cdot W = 64 \\cdot 1 \\cdot 1 = 64$.\n",
    "\n",
    "---\n",
    "\n",
    "#### Backward\n",
    "\n",
    "Der Gradient aus dem Dense Layer hat die Form `(B, 64)` und muss lediglich wieder in `(B, 64, 1, 1)` umgeformt werden:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial X} = \\mathrm{reshape}\\!\\left(\\frac{\\partial \\mathcal{L}}{\\partial Y},\\; B \\times 64 \\times 1 \\times 1\\right)\n",
    "$$\n",
    "\n",
    "Das entspricht direkt `grad_output.reshape(x.shape)` in `FlattenLayer.backward`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fd5430f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlattenLayer(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x: Tensor) -> Tensor:\n",
    "        ctx.save_for_backward(x)\n",
    "        # Flattening: (B, C, H, W) -> (B, C*H*W)\n",
    "        return x.reshape(x.size(0), -1)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output: Tensor) -> Tensor:\n",
    "        (x,) = ctx.saved_tensors\n",
    "        # Flattening rückgängig machen: (B, C*H*W) -> (B, C, H, W)\n",
    "        return grad_output.reshape(x.shape)\n",
    "\n",
    "\n",
    "class CustomFlatten(nn.Module):\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return FlattenLayer.apply(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608ee25e",
   "metadata": {},
   "source": [
    "### Dense Layer\n",
    "\n",
    "Der Dense Layer nimmt den flatten Feature-Vektor der vorherigen Schichten entgegen und berechnet eine affine Abbildung. Er fasst alle gelernten Merkmale zu einer kompakten Repräsentation zusammen und erzeugt letztendlich die finale RUL-Vorhersage.\n",
    "\n",
    "---\n",
    "\n",
    "#### Batch-Berechnung\n",
    "\n",
    "Forward-Pass\n",
    "\n",
    "$$X \\in \\mathbb{R}^{B \\times n},$$\n",
    "\n",
    "Gewichtsmatrix\n",
    "\n",
    "$$W \\in \\mathbb{R}^{n \\times m},$$\n",
    "\n",
    "und Bias\n",
    "\n",
    "$$b \\in \\mathbb{R}^{m}$$\n",
    "\n",
    "berechnet der Layer:\n",
    "\n",
    "$$Y = XW + b,$$\n",
    "\n",
    "wobei $b$ über die Batch-Dimension gebroadcastet wird. Die Ausgabe hat die Form\n",
    "\n",
    "$$Y \\in \\mathbb{R}^{B \\times m}.$$\n",
    "\n",
    "Jede Spalte von $W$ entspricht einem Neuron, der Layer besteht also aus $m$ Neuronen, die parallel berechnet werden.\n",
    "\n",
    "---\n",
    "\n",
    "#### Komponentenweise Darstellung\n",
    "\n",
    "Für jedes Sample $k \\in \\{1, \\dots, B\\}$ und jedes Ausgabe-Neuron $j \\in \\{1, \\dots, m\\}$:\n",
    "\n",
    "$$Y_{kj} = \\sum_{i=1}^{n} X_{ki} W_{ij} + b_j.$$\n",
    "\n",
    "---\n",
    "\n",
    "### Backward Pass (Gradienten)\n",
    "\n",
    "Sei $\\frac{\\partial \\mathcal{L}}{\\partial Y} \\in \\mathbb{R}^{B \\times m}$ der Gradient der Loss-Funktion bezüglich des Outputs. Daraus ergeben sich die Gradienten für Gewichte, Bias und Input:\n",
    "\n",
    "#### Gradient der Gewichte\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial W} = X^\\top \\cdot \\frac{\\partial \\mathcal{L}}{\\partial Y} \\quad \\in \\mathbb{R}^{n \\times m}$$\n",
    "\n",
    "#### Gradient des Bias\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial b} = \\sum_{k=1}^{B} \\frac{\\partial \\mathcal{L}}{\\partial Y}_{k} \\quad \\in \\mathbb{R}^{m}$$\n",
    "\n",
    "*(Summe über die Batch-Dimension)*\n",
    "\n",
    "#### Gradient des Inputs\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial X} = \\frac{\\partial \\mathcal{L}}{\\partial Y} \\cdot W^\\top \\quad \\in \\mathbb{R}^{B \\times n}$$\n",
    "\n",
    "---\n",
    "\n",
    "### Im Code:\n",
    "\n",
    "- `grad_weight = x.T @ grad_output` -> $\\frac{\\partial \\mathcal{L}}{\\partial W}$\n",
    "- `grad_bias = grad_output.sum(dim=0)` -> $\\frac{\\partial \\mathcal{L}}{\\partial b}$\n",
    "- `grad_x = grad_output @ weight.T` -> $\\frac{\\partial \\mathcal{L}}{\\partial X}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7857fbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(\n",
    "        ctx, x: Tensor, weight: Tensor, bias: Tensor\n",
    "    ) -> Tensor:\n",
    "        ctx.save_for_backward(x, weight)\n",
    "        return x @ weight + bias\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(\n",
    "        ctx, grad_output: Tensor\n",
    "    ) -> tuple[Tensor, Tensor, Tensor]:\n",
    "        x, weight = ctx.saved_tensors\n",
    "        # Gradient der Gewichte berechnen\n",
    "        grad_weight: Tensor = x.T @ grad_output\n",
    "        # Gradient des Bias berechnen\n",
    "        grad_bias: Tensor = grad_output.sum(dim=0)\n",
    "        # Gradient des Inputs berechnen\n",
    "        grad_x: Tensor = grad_output @ weight.T\n",
    "\n",
    "        return grad_x, grad_weight, grad_bias\n",
    "\n",
    "class CustomDense(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_features: int, out_features: int\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        # Random Parameterinitialisierung mit der He-Initialisierung\n",
    "        # mit der Dimension: in_features x out_features\n",
    "        self.weight: nn.Parameter = nn.Parameter(\n",
    "            torch.randn(in_features, out_features)\n",
    "            * (2 / in_features) ** 0.5\n",
    "        )\n",
    "        self.bias: nn.Parameter = nn.Parameter(torch.zeros(out_features))\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return DenseLayer.apply(x, self.weight, self.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a302f61",
   "metadata": {},
   "source": [
    "### Data Loading\n",
    "\n",
    "Im folgenden werden Trainings- und Testdaten geladen, normalisiert und zu Tensoren umgewandelt, sodass wir schlussendlich einen train- und test_loader haben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cafa0b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "data loaded\n",
      "Normalize Data...\n",
      "Data Normalized\n",
      "Create training and testing dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chrissi/programmierung/algorithmen-und-verfahren/cmapps/src/data_loader.py:174: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:256.)\n",
      "  X_train_t: torch.Tensor = torch.tensor(X_train_model, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " X_train shape: torch.Size([4906582, 10, 36])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " X_test shape: torch.Size([2735196, 10, 36])\n",
      "Datasets created!\n",
      "  Total training units:  6\n",
      "  Total test units:  4\n"
     ]
    }
   ],
   "source": [
    "from src.data_loader import (\n",
    "    load_data,\n",
    "    normalize_data,\n",
    "    create_testing_and_training_sets,\n",
    ")\n",
    "\n",
    "df_training, df_testing, new_col_names = load_data(FILENAME)\n",
    "df_training_scaled, df_testing_scaled = normalize_data(\n",
    "    df_training, df_testing, new_col_names\n",
    ")\n",
    "train_loader, test_loader = create_testing_and_training_sets(\n",
    "    df_training_scaled, df_testing_scaled\n",
    ")\n",
    "\n",
    "print(f\"  Total training units:  {df_training['unit'].nunique()}\")\n",
    "print(f\"  Total test units:  {df_testing['unit'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9af94c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model mit seinen Layern\n",
    "model: nn.Sequential = nn.Sequential(\n",
    "    # \"Layer-Block 1\"\n",
    "    CustomConv2d(in_channels=1, out_channels=64, kernel_size=3, padding=1),\n",
    "    nn.BatchNorm2d(64),\n",
    "    CustomLeakyReLU(),\n",
    "\n",
    "    # \"Layer-Block 2\"\n",
    "    CustomConv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
    "    nn.BatchNorm2d(64),\n",
    "    CustomLeakyReLU(),\n",
    "\n",
    "    # \"Layer-Block 3\"\n",
    "    CustomConv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
    "    nn.BatchNorm2d(64),\n",
    "    CustomLeakyReLU(),\n",
    "\n",
    "    # räumliche Dimensionen auf 1x1 reduzieren\n",
    "    nn.AdaptiveAvgPool2d((1, 1)),\n",
    "\n",
    "    # Fully Connected Layer, um die\n",
    "    # 64 FeatureMaps auf 1 Output zu reduzieren\n",
    "    CustomFlatten(),\n",
    "    CustomDense(64, 64),\n",
    "    CustomLeakyReLU(),\n",
    "    CustomDense(64, 1),\n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1f58f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.cnn import load_model\n",
    "\n",
    "# wenn LOAD_MODEL True ist, wird das Modell aus dem Checkpoint geladen,\n",
    "# ansonsten wird es mit den initialisierten Gewichten trainiert\n",
    "if LOAD_MODEL:\n",
    "    model: nn.Sequential = load_model(model, CHECKPOINT_PATH, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22c8afd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch 1/20...\n",
      "  Batch 2000/76666, Avg Loss: 2650.4852\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 4000/76666, Avg Loss: 2488.8505\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 6000/76666, Avg Loss: 2326.6616\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 8000/76666, Avg Loss: 2143.5809\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 10000/76666, Avg Loss: 1949.4131\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 12000/76666, Avg Loss: 1754.3148\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 14000/76666, Avg Loss: 1567.9545\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 16000/76666, Avg Loss: 1401.3840\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 18000/76666, Avg Loss: 1259.4037\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 20000/76666, Avg Loss: 1141.4169\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 22000/76666, Avg Loss: 1043.4670\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 24000/76666, Avg Loss: 961.2297\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 26000/76666, Avg Loss: 891.3383\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 28000/76666, Avg Loss: 831.2157\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 30000/76666, Avg Loss: 778.9845\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 32000/76666, Avg Loss: 733.2179\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 34000/76666, Avg Loss: 692.7157\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 36000/76666, Avg Loss: 656.6607\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 38000/76666, Avg Loss: 624.3566\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 40000/76666, Avg Loss: 595.1817\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 42000/76666, Avg Loss: 568.7994\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 44000/76666, Avg Loss: 544.7485\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 46000/76666, Avg Loss: 522.7432\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 48000/76666, Avg Loss: 502.5442\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 50000/76666, Avg Loss: 483.9231\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 52000/76666, Avg Loss: 466.7172\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 54000/76666, Avg Loss: 450.7806\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 56000/76666, Avg Loss: 435.9794\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 58000/76666, Avg Loss: 422.1899\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 60000/76666, Avg Loss: 409.2925\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 62000/76666, Avg Loss: 397.1998\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 64000/76666, Avg Loss: 385.8465\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 66000/76666, Avg Loss: 375.1852\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 68000/76666, Avg Loss: 365.1126\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 70000/76666, Avg Loss: 355.6111\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 72000/76666, Avg Loss: 346.6402\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 74000/76666, Avg Loss: 338.1416\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 76000/76666, Avg Loss: 330.0753\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "Epoch 1/20 Complete, Train MSE=327.4844, Test MSE=35.8034\n",
      "          R2-Score: 0.9402,           Accuracy: 0.0741\n",
      "\n",
      "Starting Epoch 2/20...\n",
      "  Batch 2000/76666, Avg Loss: 31.3175\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 4000/76666, Avg Loss: 31.0770\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 6000/76666, Avg Loss: 30.8597\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 8000/76666, Avg Loss: 30.6938\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 10000/76666, Avg Loss: 30.6197\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 12000/76666, Avg Loss: 30.4816\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 14000/76666, Avg Loss: 30.4344\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 16000/76666, Avg Loss: 30.2913\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 18000/76666, Avg Loss: 30.1533\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 20000/76666, Avg Loss: 29.9868\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 22000/76666, Avg Loss: 29.8043\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 24000/76666, Avg Loss: 29.6829\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 26000/76666, Avg Loss: 29.5500\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 28000/76666, Avg Loss: 29.4041\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 30000/76666, Avg Loss: 29.2457\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 32000/76666, Avg Loss: 29.1274\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 34000/76666, Avg Loss: 28.9861\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 36000/76666, Avg Loss: 28.8900\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 38000/76666, Avg Loss: 28.7553\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 40000/76666, Avg Loss: 28.6301\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 42000/76666, Avg Loss: 28.5257\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 44000/76666, Avg Loss: 28.4221\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 46000/76666, Avg Loss: 28.3086\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 48000/76666, Avg Loss: 28.2054\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 50000/76666, Avg Loss: 28.1059\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 52000/76666, Avg Loss: 27.9776\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 54000/76666, Avg Loss: 27.8709\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 56000/76666, Avg Loss: 27.7542\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 58000/76666, Avg Loss: 27.6595\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 60000/76666, Avg Loss: 27.5711\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 62000/76666, Avg Loss: 27.4827\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 64000/76666, Avg Loss: 27.3683\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 66000/76666, Avg Loss: 27.2708\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 68000/76666, Avg Loss: 27.1790\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 70000/76666, Avg Loss: 27.0986\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 72000/76666, Avg Loss: 27.0161\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 74000/76666, Avg Loss: 26.9225\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 76000/76666, Avg Loss: 26.8363\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "Epoch 2/20 Complete, Train MSE=26.8040, Test MSE=46.7956\n",
      "          R2-Score: 0.9218,           Accuracy: 0.1383\n",
      "\n",
      "Starting Epoch 3/20...\n",
      "  Batch 2000/76666, Avg Loss: 23.2338\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 4000/76666, Avg Loss: 23.1433\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 6000/76666, Avg Loss: 23.2346\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 8000/76666, Avg Loss: 23.1744\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 10000/76666, Avg Loss: 23.0638\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 12000/76666, Avg Loss: 22.9972\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 14000/76666, Avg Loss: 22.9465\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 16000/76666, Avg Loss: 22.8645\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 18000/76666, Avg Loss: 22.8383\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 20000/76666, Avg Loss: 22.7933\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 22000/76666, Avg Loss: 22.7281\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 24000/76666, Avg Loss: 22.6816\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 26000/76666, Avg Loss: 22.6372\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 28000/76666, Avg Loss: 22.5640\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 30000/76666, Avg Loss: 22.5270\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 32000/76666, Avg Loss: 22.4711\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 34000/76666, Avg Loss: 22.4144\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 36000/76666, Avg Loss: 22.3674\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 38000/76666, Avg Loss: 22.3127\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 40000/76666, Avg Loss: 22.2525\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 42000/76666, Avg Loss: 22.2006\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 44000/76666, Avg Loss: 22.1599\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 46000/76666, Avg Loss: 22.1025\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 48000/76666, Avg Loss: 22.0538\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 50000/76666, Avg Loss: 21.9982\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 52000/76666, Avg Loss: 21.9560\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 54000/76666, Avg Loss: 21.9027\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 56000/76666, Avg Loss: 21.8519\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 58000/76666, Avg Loss: 21.8127\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 60000/76666, Avg Loss: 21.7657\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 62000/76666, Avg Loss: 21.7152\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 64000/76666, Avg Loss: 21.6639\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 66000/76666, Avg Loss: 21.6178\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 68000/76666, Avg Loss: 21.5738\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 70000/76666, Avg Loss: 21.5249\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 72000/76666, Avg Loss: 21.4739\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 74000/76666, Avg Loss: 21.4254\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 76000/76666, Avg Loss: 21.3717\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "Epoch 3/20 Complete, Train MSE=21.3584, Test MSE=45.1784\n",
      "          R2-Score: 0.9245,           Accuracy: 0.1255\n",
      "\n",
      "Starting Epoch 4/20...\n",
      "  Batch 2000/76666, Avg Loss: 19.3668\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 4000/76666, Avg Loss: 19.4319\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 6000/76666, Avg Loss: 19.3707\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 8000/76666, Avg Loss: 19.3087\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 10000/76666, Avg Loss: 19.3313\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 12000/76666, Avg Loss: 19.2853\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 14000/76666, Avg Loss: 19.2656\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 16000/76666, Avg Loss: 19.2423\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 18000/76666, Avg Loss: 19.2042\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 20000/76666, Avg Loss: 19.1612\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 22000/76666, Avg Loss: 19.1522\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 24000/76666, Avg Loss: 19.1029\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 26000/76666, Avg Loss: 19.0601\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 28000/76666, Avg Loss: 19.0287\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 30000/76666, Avg Loss: 18.9951\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 32000/76666, Avg Loss: 18.9666\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 34000/76666, Avg Loss: 18.9481\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 36000/76666, Avg Loss: 18.9114\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 38000/76666, Avg Loss: 18.8753\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 40000/76666, Avg Loss: 18.8492\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 42000/76666, Avg Loss: 18.8184\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 44000/76666, Avg Loss: 18.7905\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 46000/76666, Avg Loss: 18.7539\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 48000/76666, Avg Loss: 18.7273\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 50000/76666, Avg Loss: 18.6970\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 52000/76666, Avg Loss: 18.6580\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 54000/76666, Avg Loss: 18.6186\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 56000/76666, Avg Loss: 18.5866\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 58000/76666, Avg Loss: 18.5671\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 60000/76666, Avg Loss: 18.5324\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 62000/76666, Avg Loss: 18.5043\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 64000/76666, Avg Loss: 18.4768\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 66000/76666, Avg Loss: 18.4460\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 68000/76666, Avg Loss: 18.4159\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 70000/76666, Avg Loss: 18.3835\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 72000/76666, Avg Loss: 18.3547\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 74000/76666, Avg Loss: 18.3197\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 76000/76666, Avg Loss: 18.2899\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "Epoch 4/20 Complete, Train MSE=18.2788, Test MSE=43.9782\n",
      "          R2-Score: 0.9266,           Accuracy: 0.1265\n",
      "\n",
      "Starting Epoch 5/20...\n",
      "  Batch 2000/76666, Avg Loss: 17.0676\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 4000/76666, Avg Loss: 16.9547\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 6000/76666, Avg Loss: 16.9529\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 8000/76666, Avg Loss: 16.9040\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 10000/76666, Avg Loss: 16.9056\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 12000/76666, Avg Loss: 16.8787\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 14000/76666, Avg Loss: 16.8641\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 16000/76666, Avg Loss: 16.8466\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 18000/76666, Avg Loss: 16.8207\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 20000/76666, Avg Loss: 16.7965\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 22000/76666, Avg Loss: 16.7641\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 24000/76666, Avg Loss: 16.7416\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 26000/76666, Avg Loss: 16.7122\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 28000/76666, Avg Loss: 16.6846\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 30000/76666, Avg Loss: 16.6628\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 32000/76666, Avg Loss: 16.6325\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 34000/76666, Avg Loss: 16.6131\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 36000/76666, Avg Loss: 16.5803\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 38000/76666, Avg Loss: 16.5495\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 40000/76666, Avg Loss: 16.5275\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 42000/76666, Avg Loss: 16.5026\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 44000/76666, Avg Loss: 16.4932\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 46000/76666, Avg Loss: 16.4650\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 48000/76666, Avg Loss: 16.4473\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 50000/76666, Avg Loss: 16.4204\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 52000/76666, Avg Loss: 16.3930\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 54000/76666, Avg Loss: 16.3618\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 56000/76666, Avg Loss: 16.3431\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 58000/76666, Avg Loss: 16.3222\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 60000/76666, Avg Loss: 16.2969\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 62000/76666, Avg Loss: 16.2695\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 64000/76666, Avg Loss: 16.2480\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 66000/76666, Avg Loss: 16.2277\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 68000/76666, Avg Loss: 16.2083\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 70000/76666, Avg Loss: 16.1836\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 72000/76666, Avg Loss: 16.1652\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 74000/76666, Avg Loss: 16.1394\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 76000/76666, Avg Loss: 16.1201\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "Epoch 5/20 Complete, Train MSE=16.1152, Test MSE=46.1504\n",
      "          R2-Score: 0.9229,           Accuracy: 0.1026\n",
      "\n",
      "Starting Epoch 6/20...\n",
      "  Batch 2000/76666, Avg Loss: 15.2855\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 4000/76666, Avg Loss: 15.2553\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 6000/76666, Avg Loss: 15.2494\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 8000/76666, Avg Loss: 15.2565\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 10000/76666, Avg Loss: 15.2452\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 12000/76666, Avg Loss: 15.2342\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 14000/76666, Avg Loss: 15.1940\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 16000/76666, Avg Loss: 15.1651\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 18000/76666, Avg Loss: 15.1431\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 20000/76666, Avg Loss: 15.1337\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 22000/76666, Avg Loss: 15.1129\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 24000/76666, Avg Loss: 15.1051\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 26000/76666, Avg Loss: 15.0929\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 28000/76666, Avg Loss: 15.0712\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 30000/76666, Avg Loss: 15.0539\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 32000/76666, Avg Loss: 15.0476\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 34000/76666, Avg Loss: 15.0336\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 36000/76666, Avg Loss: 15.0236\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 38000/76666, Avg Loss: 15.0189\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 40000/76666, Avg Loss: 15.0077\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 42000/76666, Avg Loss: 14.9995\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 44000/76666, Avg Loss: 14.9922\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 46000/76666, Avg Loss: 14.9774\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 48000/76666, Avg Loss: 14.9675\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 50000/76666, Avg Loss: 14.9606\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 52000/76666, Avg Loss: 14.9531\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 54000/76666, Avg Loss: 14.9400\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 56000/76666, Avg Loss: 14.9307\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 58000/76666, Avg Loss: 14.9221\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 60000/76666, Avg Loss: 14.9127\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 62000/76666, Avg Loss: 14.9064\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 64000/76666, Avg Loss: 14.8945\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 66000/76666, Avg Loss: 14.8874\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 68000/76666, Avg Loss: 14.8826\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 70000/76666, Avg Loss: 14.8771\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 72000/76666, Avg Loss: 14.8705\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 74000/76666, Avg Loss: 14.8632\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 76000/76666, Avg Loss: 14.8587\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "Epoch 6/20 Complete, Train MSE=14.8562, Test MSE=44.7475\n",
      "          R2-Score: 0.9253,           Accuracy: 0.0967\n",
      "\n",
      "Starting Epoch 7/20...\n",
      "  Batch 2000/76666, Avg Loss: 14.5263\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 4000/76666, Avg Loss: 14.5254\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 6000/76666, Avg Loss: 14.5354\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 8000/76666, Avg Loss: 14.5182\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 10000/76666, Avg Loss: 14.5345\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 12000/76666, Avg Loss: 14.5283\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 14000/76666, Avg Loss: 14.5071\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 16000/76666, Avg Loss: 14.4916\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 18000/76666, Avg Loss: 14.5048\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 20000/76666, Avg Loss: 14.4900\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 22000/76666, Avg Loss: 14.4768\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 24000/76666, Avg Loss: 14.4617\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 26000/76666, Avg Loss: 14.4585\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 28000/76666, Avg Loss: 14.4487\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 30000/76666, Avg Loss: 14.4460\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 32000/76666, Avg Loss: 14.4442\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 34000/76666, Avg Loss: 14.4358\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 36000/76666, Avg Loss: 14.4257\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 38000/76666, Avg Loss: 14.4160\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 40000/76666, Avg Loss: 14.4127\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 42000/76666, Avg Loss: 14.4090\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 44000/76666, Avg Loss: 14.4000\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 46000/76666, Avg Loss: 14.3972\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 48000/76666, Avg Loss: 14.3885\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 50000/76666, Avg Loss: 14.3817\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 52000/76666, Avg Loss: 14.3766\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 54000/76666, Avg Loss: 14.3673\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 56000/76666, Avg Loss: 14.3614\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 58000/76666, Avg Loss: 14.3503\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 60000/76666, Avg Loss: 14.3432\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 62000/76666, Avg Loss: 14.3363\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 64000/76666, Avg Loss: 14.3327\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 66000/76666, Avg Loss: 14.3267\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 68000/76666, Avg Loss: 14.3194\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 70000/76666, Avg Loss: 14.3140\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 72000/76666, Avg Loss: 14.3061\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 74000/76666, Avg Loss: 14.2995\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 76000/76666, Avg Loss: 14.2929\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "Epoch 7/20 Complete, Train MSE=14.2890, Test MSE=45.3246\n",
      "          R2-Score: 0.9243,           Accuracy: 0.1026\n",
      "\n",
      "Starting Epoch 8/20...\n",
      "  Batch 2000/76666, Avg Loss: 13.9906\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 4000/76666, Avg Loss: 14.0432\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 6000/76666, Avg Loss: 14.0106\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 8000/76666, Avg Loss: 14.0228\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 10000/76666, Avg Loss: 13.9990\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 12000/76666, Avg Loss: 14.0106\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 14000/76666, Avg Loss: 14.0177\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 16000/76666, Avg Loss: 14.0063\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 18000/76666, Avg Loss: 13.9947\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 20000/76666, Avg Loss: 13.9810\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 22000/76666, Avg Loss: 13.9773\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 24000/76666, Avg Loss: 13.9805\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 26000/76666, Avg Loss: 13.9606\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 28000/76666, Avg Loss: 13.9693\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 30000/76666, Avg Loss: 13.9617\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 32000/76666, Avg Loss: 13.9512\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 34000/76666, Avg Loss: 13.9422\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 36000/76666, Avg Loss: 13.9364\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 38000/76666, Avg Loss: 13.9285\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 40000/76666, Avg Loss: 13.9294\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 42000/76666, Avg Loss: 13.9256\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 44000/76666, Avg Loss: 13.9216\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 46000/76666, Avg Loss: 13.9150\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 48000/76666, Avg Loss: 13.9103\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 50000/76666, Avg Loss: 13.9046\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 52000/76666, Avg Loss: 13.8953\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 54000/76666, Avg Loss: 13.8857\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 56000/76666, Avg Loss: 13.8813\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 58000/76666, Avg Loss: 13.8735\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 60000/76666, Avg Loss: 13.8647\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 62000/76666, Avg Loss: 13.8583\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 64000/76666, Avg Loss: 13.8545\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 66000/76666, Avg Loss: 13.8489\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 68000/76666, Avg Loss: 13.8400\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 70000/76666, Avg Loss: 13.8319\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 72000/76666, Avg Loss: 13.8247\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 74000/76666, Avg Loss: 13.8178\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 76000/76666, Avg Loss: 13.8121\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "Epoch 8/20 Complete, Train MSE=13.8106, Test MSE=50.3178\n",
      "          R2-Score: 0.9160,           Accuracy: 0.1056\n",
      "\n",
      "Starting Epoch 9/20...\n",
      "  Batch 2000/76666, Avg Loss: 13.5287\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 4000/76666, Avg Loss: 13.5854\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 6000/76666, Avg Loss: 13.6178\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 8000/76666, Avg Loss: 13.6086\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 10000/76666, Avg Loss: 13.6105\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 12000/76666, Avg Loss: 13.6047\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 14000/76666, Avg Loss: 13.6028\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 16000/76666, Avg Loss: 13.6004\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 18000/76666, Avg Loss: 13.5890\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 20000/76666, Avg Loss: 13.5691\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 22000/76666, Avg Loss: 13.5579\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 24000/76666, Avg Loss: 13.5556\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 26000/76666, Avg Loss: 13.5615\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 28000/76666, Avg Loss: 13.5437\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 30000/76666, Avg Loss: 13.5335\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 32000/76666, Avg Loss: 13.5314\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 34000/76666, Avg Loss: 13.5264\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 36000/76666, Avg Loss: 13.5233\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 38000/76666, Avg Loss: 13.5119\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 40000/76666, Avg Loss: 13.5078\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 42000/76666, Avg Loss: 13.5023\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 44000/76666, Avg Loss: 13.4980\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 46000/76666, Avg Loss: 13.4937\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 48000/76666, Avg Loss: 13.4874\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 50000/76666, Avg Loss: 13.4827\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 52000/76666, Avg Loss: 13.4771\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 54000/76666, Avg Loss: 13.4763\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 56000/76666, Avg Loss: 13.4697\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 58000/76666, Avg Loss: 13.4698\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 60000/76666, Avg Loss: 13.4672\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 62000/76666, Avg Loss: 13.4623\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 64000/76666, Avg Loss: 13.4592\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 66000/76666, Avg Loss: 13.4542\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 68000/76666, Avg Loss: 13.4517\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 70000/76666, Avg Loss: 13.4462\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 72000/76666, Avg Loss: 13.4439\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 74000/76666, Avg Loss: 13.4379\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 76000/76666, Avg Loss: 13.4333\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "Epoch 9/20 Complete, Train MSE=13.4321, Test MSE=50.5696\n",
      "          R2-Score: 0.9155,           Accuracy: 0.1007\n",
      "\n",
      "Starting Epoch 10/20...\n",
      "  Batch 2000/76666, Avg Loss: 13.3488\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 4000/76666, Avg Loss: 13.2887\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 6000/76666, Avg Loss: 13.2379\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 8000/76666, Avg Loss: 13.2354\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 10000/76666, Avg Loss: 13.2285\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 12000/76666, Avg Loss: 13.2190\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 14000/76666, Avg Loss: 13.2162\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 16000/76666, Avg Loss: 13.2157\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 18000/76666, Avg Loss: 13.2002\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 20000/76666, Avg Loss: 13.2024\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 22000/76666, Avg Loss: 13.1954\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 24000/76666, Avg Loss: 13.1968\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 26000/76666, Avg Loss: 13.1848\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 28000/76666, Avg Loss: 13.1831\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 30000/76666, Avg Loss: 13.1784\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 32000/76666, Avg Loss: 13.1749\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 34000/76666, Avg Loss: 13.1659\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 36000/76666, Avg Loss: 13.1648\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 38000/76666, Avg Loss: 13.1557\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 40000/76666, Avg Loss: 13.1549\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 42000/76666, Avg Loss: 13.1538\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 44000/76666, Avg Loss: 13.1547\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 46000/76666, Avg Loss: 13.1512\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 48000/76666, Avg Loss: 13.1484\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 50000/76666, Avg Loss: 13.1479\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 52000/76666, Avg Loss: 13.1465\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 54000/76666, Avg Loss: 13.1436\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 56000/76666, Avg Loss: 13.1422\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 58000/76666, Avg Loss: 13.1378\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 60000/76666, Avg Loss: 13.1374\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 62000/76666, Avg Loss: 13.1347\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 64000/76666, Avg Loss: 13.1322\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 66000/76666, Avg Loss: 13.1307\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 68000/76666, Avg Loss: 13.1278\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 70000/76666, Avg Loss: 13.1234\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 72000/76666, Avg Loss: 13.1224\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 74000/76666, Avg Loss: 13.1186\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 76000/76666, Avg Loss: 13.1195\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "Epoch 10/20 Complete, Train MSE=13.1200, Test MSE=50.6897\n",
      "          R2-Score: 0.9153,           Accuracy: 0.1074\n",
      "\n",
      "Starting Epoch 11/20...\n",
      "  Batch 2000/76666, Avg Loss: 13.0983\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 4000/76666, Avg Loss: 13.0295\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 6000/76666, Avg Loss: 13.0018\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 8000/76666, Avg Loss: 13.0242\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 10000/76666, Avg Loss: 13.0200\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 12000/76666, Avg Loss: 13.0233\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 14000/76666, Avg Loss: 13.0085\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 16000/76666, Avg Loss: 13.0056\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 18000/76666, Avg Loss: 13.0054\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 20000/76666, Avg Loss: 13.0011\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 22000/76666, Avg Loss: 13.0013\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 24000/76666, Avg Loss: 13.0012\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 26000/76666, Avg Loss: 13.0051\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 28000/76666, Avg Loss: 13.0023\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 30000/76666, Avg Loss: 12.9983\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 32000/76666, Avg Loss: 13.0036\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 34000/76666, Avg Loss: 13.0017\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 36000/76666, Avg Loss: 13.0000\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 38000/76666, Avg Loss: 13.0010\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 40000/76666, Avg Loss: 13.0002\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 42000/76666, Avg Loss: 13.0014\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 44000/76666, Avg Loss: 12.9994\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 46000/76666, Avg Loss: 12.9944\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 48000/76666, Avg Loss: 12.9876\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 50000/76666, Avg Loss: 12.9868\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 52000/76666, Avg Loss: 12.9873\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 54000/76666, Avg Loss: 12.9836\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 56000/76666, Avg Loss: 12.9826\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 58000/76666, Avg Loss: 12.9807\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 60000/76666, Avg Loss: 12.9802\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 62000/76666, Avg Loss: 12.9781\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 64000/76666, Avg Loss: 12.9724\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 66000/76666, Avg Loss: 12.9724\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 68000/76666, Avg Loss: 12.9672\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 70000/76666, Avg Loss: 12.9661\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 72000/76666, Avg Loss: 12.9616\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 74000/76666, Avg Loss: 12.9625\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 76000/76666, Avg Loss: 12.9589\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "Epoch 11/20 Complete, Train MSE=12.9576, Test MSE=45.4731\n",
      "          R2-Score: 0.9241,           Accuracy: 0.0857\n",
      "\n",
      "Starting Epoch 12/20...\n",
      "  Batch 2000/76666, Avg Loss: 12.9219\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 4000/76666, Avg Loss: 12.9529\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 6000/76666, Avg Loss: 12.9545\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 8000/76666, Avg Loss: 12.9334\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 10000/76666, Avg Loss: 12.9235\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 12000/76666, Avg Loss: 12.9176\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 14000/76666, Avg Loss: 12.8998\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 16000/76666, Avg Loss: 12.8934\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 18000/76666, Avg Loss: 12.8936\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 20000/76666, Avg Loss: 12.8924\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 22000/76666, Avg Loss: 12.8908\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 24000/76666, Avg Loss: 12.8895\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 26000/76666, Avg Loss: 12.8942\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 28000/76666, Avg Loss: 12.8961\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 30000/76666, Avg Loss: 12.8939\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 32000/76666, Avg Loss: 12.8915\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 34000/76666, Avg Loss: 12.8920\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 36000/76666, Avg Loss: 12.8779\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 38000/76666, Avg Loss: 12.8698\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 40000/76666, Avg Loss: 12.8619\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 42000/76666, Avg Loss: 12.8597\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 44000/76666, Avg Loss: 12.8570\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 46000/76666, Avg Loss: 12.8550\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 48000/76666, Avg Loss: 12.8516\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 50000/76666, Avg Loss: 12.8485\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 52000/76666, Avg Loss: 12.8485\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 54000/76666, Avg Loss: 12.8389\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 56000/76666, Avg Loss: 12.8400\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 58000/76666, Avg Loss: 12.8390\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 60000/76666, Avg Loss: 12.8390\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 62000/76666, Avg Loss: 12.8396\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 64000/76666, Avg Loss: 12.8373\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 66000/76666, Avg Loss: 12.8314\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 68000/76666, Avg Loss: 12.8296\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 70000/76666, Avg Loss: 12.8284\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 72000/76666, Avg Loss: 12.8229\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 74000/76666, Avg Loss: 12.8195\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 76000/76666, Avg Loss: 12.8179\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "Epoch 12/20 Complete, Train MSE=12.8172, Test MSE=50.6446\n",
      "          R2-Score: 0.9154,           Accuracy: 0.0984\n",
      "\n",
      "Starting Epoch 13/20...\n",
      "  Batch 2000/76666, Avg Loss: 12.7931\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 4000/76666, Avg Loss: 12.7466\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 6000/76666, Avg Loss: 12.7384\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 8000/76666, Avg Loss: 12.7333\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 10000/76666, Avg Loss: 12.7260\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 12000/76666, Avg Loss: 12.7193\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 14000/76666, Avg Loss: 12.7186\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 16000/76666, Avg Loss: 12.7208\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 18000/76666, Avg Loss: 12.7208\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 20000/76666, Avg Loss: 12.7197\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 22000/76666, Avg Loss: 12.7195\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 24000/76666, Avg Loss: 12.7188\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 26000/76666, Avg Loss: 12.7127\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 28000/76666, Avg Loss: 12.7177\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 30000/76666, Avg Loss: 12.7172\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 32000/76666, Avg Loss: 12.7180\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 34000/76666, Avg Loss: 12.7203\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 36000/76666, Avg Loss: 12.7250\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 38000/76666, Avg Loss: 12.7242\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 40000/76666, Avg Loss: 12.7229\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 42000/76666, Avg Loss: 12.7204\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 44000/76666, Avg Loss: 12.7174\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 46000/76666, Avg Loss: 12.7150\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 48000/76666, Avg Loss: 12.7121\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 50000/76666, Avg Loss: 12.7109\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 52000/76666, Avg Loss: 12.7085\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 54000/76666, Avg Loss: 12.7086\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 56000/76666, Avg Loss: 12.7111\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 58000/76666, Avg Loss: 12.7076\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 60000/76666, Avg Loss: 12.7023\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 62000/76666, Avg Loss: 12.7014\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 64000/76666, Avg Loss: 12.6986\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 66000/76666, Avg Loss: 12.6982\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 68000/76666, Avg Loss: 12.6981\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 70000/76666, Avg Loss: 12.6959\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 72000/76666, Avg Loss: 12.6961\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 74000/76666, Avg Loss: 12.6939\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 76000/76666, Avg Loss: 12.6917\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "Epoch 13/20 Complete, Train MSE=12.6910, Test MSE=46.9071\n",
      "          R2-Score: 0.9217,           Accuracy: 0.0871\n",
      "\n",
      "Starting Epoch 14/20...\n",
      "  Batch 2000/76666, Avg Loss: 12.4998\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 4000/76666, Avg Loss: 12.4809\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 6000/76666, Avg Loss: 12.5208\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 8000/76666, Avg Loss: 12.5217\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 10000/76666, Avg Loss: 12.5260\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 12000/76666, Avg Loss: 12.5311\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 14000/76666, Avg Loss: 12.5301\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 16000/76666, Avg Loss: 12.5451\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 18000/76666, Avg Loss: 12.5554\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 20000/76666, Avg Loss: 12.5454\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 22000/76666, Avg Loss: 12.5479\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 24000/76666, Avg Loss: 12.5524\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 26000/76666, Avg Loss: 12.5588\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 28000/76666, Avg Loss: 12.5649\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 30000/76666, Avg Loss: 12.5650\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 32000/76666, Avg Loss: 12.5640\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 34000/76666, Avg Loss: 12.5655\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 36000/76666, Avg Loss: 12.5688\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 38000/76666, Avg Loss: 12.5640\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 40000/76666, Avg Loss: 12.5656\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 42000/76666, Avg Loss: 12.5645\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 44000/76666, Avg Loss: 12.5671\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 46000/76666, Avg Loss: 12.5690\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 48000/76666, Avg Loss: 12.5659\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 50000/76666, Avg Loss: 12.5644\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 52000/76666, Avg Loss: 12.5641\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 54000/76666, Avg Loss: 12.5649\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 56000/76666, Avg Loss: 12.5671\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 58000/76666, Avg Loss: 12.5698\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 60000/76666, Avg Loss: 12.5705\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 62000/76666, Avg Loss: 12.5691\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 64000/76666, Avg Loss: 12.5703\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 66000/76666, Avg Loss: 12.5671\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 68000/76666, Avg Loss: 12.5680\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 70000/76666, Avg Loss: 12.5676\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 72000/76666, Avg Loss: 12.5674\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 74000/76666, Avg Loss: 12.5660\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 76000/76666, Avg Loss: 12.5626\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "Epoch 14/20 Complete, Train MSE=12.5630, Test MSE=47.4191\n",
      "          R2-Score: 0.9208,           Accuracy: 0.0876\n",
      "\n",
      "Starting Epoch 15/20...\n",
      "  Batch 2000/76666, Avg Loss: 12.5024\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 4000/76666, Avg Loss: 12.4726\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 6000/76666, Avg Loss: 12.5114\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 8000/76666, Avg Loss: 12.5178\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 10000/76666, Avg Loss: 12.5164\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 12000/76666, Avg Loss: 12.5121\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 14000/76666, Avg Loss: 12.5095\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 16000/76666, Avg Loss: 12.5270\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 18000/76666, Avg Loss: 12.5290\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 20000/76666, Avg Loss: 12.5162\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 22000/76666, Avg Loss: 12.5225\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 24000/76666, Avg Loss: 12.5242\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 26000/76666, Avg Loss: 12.5240\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 28000/76666, Avg Loss: 12.5289\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 30000/76666, Avg Loss: 12.5310\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 32000/76666, Avg Loss: 12.5286\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 34000/76666, Avg Loss: 12.5331\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 36000/76666, Avg Loss: 12.5294\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 38000/76666, Avg Loss: 12.5327\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 40000/76666, Avg Loss: 12.5321\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 42000/76666, Avg Loss: 12.5245\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 44000/76666, Avg Loss: 12.5211\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 46000/76666, Avg Loss: 12.5161\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 48000/76666, Avg Loss: 12.5140\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 50000/76666, Avg Loss: 12.5132\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 52000/76666, Avg Loss: 12.5136\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 54000/76666, Avg Loss: 12.5175\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 56000/76666, Avg Loss: 12.5206\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 58000/76666, Avg Loss: 12.5189\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 60000/76666, Avg Loss: 12.5183\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 62000/76666, Avg Loss: 12.5163\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 64000/76666, Avg Loss: 12.5136\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 66000/76666, Avg Loss: 12.5115\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 68000/76666, Avg Loss: 12.5100\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 70000/76666, Avg Loss: 12.5094\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 72000/76666, Avg Loss: 12.5078\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 74000/76666, Avg Loss: 12.5069\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 76000/76666, Avg Loss: 12.5106\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "Epoch 15/20 Complete, Train MSE=12.5107, Test MSE=46.3869\n",
      "          R2-Score: 0.9225,           Accuracy: 0.0943\n",
      "\n",
      "Starting Epoch 16/20...\n",
      "  Batch 2000/76666, Avg Loss: 12.4671\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 4000/76666, Avg Loss: 12.4629\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 6000/76666, Avg Loss: 12.4916\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 8000/76666, Avg Loss: 12.4799\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 10000/76666, Avg Loss: 12.4780\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 12000/76666, Avg Loss: 12.4870\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 14000/76666, Avg Loss: 12.4816\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 16000/76666, Avg Loss: 12.4766\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 18000/76666, Avg Loss: 12.4755\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 20000/76666, Avg Loss: 12.4760\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 22000/76666, Avg Loss: 12.4777\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 24000/76666, Avg Loss: 12.4796\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 26000/76666, Avg Loss: 12.4775\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 28000/76666, Avg Loss: 12.4764\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 30000/76666, Avg Loss: 12.4664\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 32000/76666, Avg Loss: 12.4673\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 34000/76666, Avg Loss: 12.4657\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 36000/76666, Avg Loss: 12.4646\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 38000/76666, Avg Loss: 12.4643\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 40000/76666, Avg Loss: 12.4636\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 42000/76666, Avg Loss: 12.4655\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 44000/76666, Avg Loss: 12.4646\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 46000/76666, Avg Loss: 12.4666\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 48000/76666, Avg Loss: 12.4678\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 50000/76666, Avg Loss: 12.4666\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 52000/76666, Avg Loss: 12.4653\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 54000/76666, Avg Loss: 12.4641\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 56000/76666, Avg Loss: 12.4672\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 58000/76666, Avg Loss: 12.4648\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 60000/76666, Avg Loss: 12.4637\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 62000/76666, Avg Loss: 12.4618\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 64000/76666, Avg Loss: 12.4587\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 66000/76666, Avg Loss: 12.4566\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 68000/76666, Avg Loss: 12.4541\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 70000/76666, Avg Loss: 12.4555\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 72000/76666, Avg Loss: 12.4552\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 74000/76666, Avg Loss: 12.4525\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 76000/76666, Avg Loss: 12.4499\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "Epoch 16/20 Complete, Train MSE=12.4504, Test MSE=46.3159\n",
      "          R2-Score: 0.9226,           Accuracy: 0.0906\n",
      "\n",
      "Starting Epoch 17/20...\n",
      "  Batch 2000/76666, Avg Loss: 12.4675\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 4000/76666, Avg Loss: 12.5066\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 6000/76666, Avg Loss: 12.4701\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 8000/76666, Avg Loss: 12.4626\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 10000/76666, Avg Loss: 12.4479\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 12000/76666, Avg Loss: 12.4374\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 14000/76666, Avg Loss: 12.4447\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 16000/76666, Avg Loss: 12.4617\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 18000/76666, Avg Loss: 12.4629\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 20000/76666, Avg Loss: 12.4487\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 22000/76666, Avg Loss: 12.4439\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 24000/76666, Avg Loss: 12.4365\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 26000/76666, Avg Loss: 12.4289\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 28000/76666, Avg Loss: 12.4302\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 30000/76666, Avg Loss: 12.4344\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 32000/76666, Avg Loss: 12.4364\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 34000/76666, Avg Loss: 12.4352\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 36000/76666, Avg Loss: 12.4329\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 38000/76666, Avg Loss: 12.4344\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 40000/76666, Avg Loss: 12.4285\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 42000/76666, Avg Loss: 12.4242\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 44000/76666, Avg Loss: 12.4254\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 46000/76666, Avg Loss: 12.4242\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 48000/76666, Avg Loss: 12.4222\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 50000/76666, Avg Loss: 12.4171\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 52000/76666, Avg Loss: 12.4174\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 54000/76666, Avg Loss: 12.4144\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 56000/76666, Avg Loss: 12.4161\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 58000/76666, Avg Loss: 12.4134\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 60000/76666, Avg Loss: 12.4140\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 62000/76666, Avg Loss: 12.4106\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 64000/76666, Avg Loss: 12.4064\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 66000/76666, Avg Loss: 12.4041\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 68000/76666, Avg Loss: 12.4033\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 70000/76666, Avg Loss: 12.4025\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 72000/76666, Avg Loss: 12.4019\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 74000/76666, Avg Loss: 12.4026\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 76000/76666, Avg Loss: 12.4012\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "Epoch 17/20 Complete, Train MSE=12.4018, Test MSE=50.6758\n",
      "          R2-Score: 0.9154,           Accuracy: 0.0978\n",
      "\n",
      "Starting Epoch 18/20...\n",
      "  Batch 2000/76666, Avg Loss: 12.3153\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 4000/76666, Avg Loss: 12.3928\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 6000/76666, Avg Loss: 12.3243\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 8000/76666, Avg Loss: 12.3588\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 10000/76666, Avg Loss: 12.3596\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 12000/76666, Avg Loss: 12.3504\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 14000/76666, Avg Loss: 12.3439\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 16000/76666, Avg Loss: 12.3542\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 18000/76666, Avg Loss: 12.3479\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 20000/76666, Avg Loss: 12.3455\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 22000/76666, Avg Loss: 12.3494\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 24000/76666, Avg Loss: 12.3484\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 26000/76666, Avg Loss: 12.3454\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 28000/76666, Avg Loss: 12.3422\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 30000/76666, Avg Loss: 12.3381\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 32000/76666, Avg Loss: 12.3438\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 34000/76666, Avg Loss: 12.3420\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 36000/76666, Avg Loss: 12.3395\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 38000/76666, Avg Loss: 12.3393\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 40000/76666, Avg Loss: 12.3411\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 42000/76666, Avg Loss: 12.3452\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 44000/76666, Avg Loss: 12.3486\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 46000/76666, Avg Loss: 12.3478\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 48000/76666, Avg Loss: 12.3447\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 50000/76666, Avg Loss: 12.3460\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 52000/76666, Avg Loss: 12.3451\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 54000/76666, Avg Loss: 12.3460\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 56000/76666, Avg Loss: 12.3452\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 58000/76666, Avg Loss: 12.3459\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 60000/76666, Avg Loss: 12.3469\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 62000/76666, Avg Loss: 12.3451\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 64000/76666, Avg Loss: 12.3516\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 66000/76666, Avg Loss: 12.3496\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 68000/76666, Avg Loss: 12.3441\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 70000/76666, Avg Loss: 12.3460\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 72000/76666, Avg Loss: 12.3459\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 74000/76666, Avg Loss: 12.3476\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 76000/76666, Avg Loss: 12.3466\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "Epoch 18/20 Complete, Train MSE=12.3463, Test MSE=46.6365\n",
      "          R2-Score: 0.9221,           Accuracy: 0.0935\n",
      "\n",
      "Starting Epoch 19/20...\n",
      "  Batch 2000/76666, Avg Loss: 12.3026\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 4000/76666, Avg Loss: 12.2658\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 6000/76666, Avg Loss: 12.3012\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 8000/76666, Avg Loss: 12.3043\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 10000/76666, Avg Loss: 12.3325\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 12000/76666, Avg Loss: 12.3402\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 14000/76666, Avg Loss: 12.3528\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 16000/76666, Avg Loss: 12.3536\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 18000/76666, Avg Loss: 12.3510\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 20000/76666, Avg Loss: 12.3434\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 22000/76666, Avg Loss: 12.3489\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 24000/76666, Avg Loss: 12.3491\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 26000/76666, Avg Loss: 12.3456\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 28000/76666, Avg Loss: 12.3422\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 30000/76666, Avg Loss: 12.3484\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 32000/76666, Avg Loss: 12.3475\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 34000/76666, Avg Loss: 12.3402\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 36000/76666, Avg Loss: 12.3410\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 38000/76666, Avg Loss: 12.3394\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 40000/76666, Avg Loss: 12.3394\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 42000/76666, Avg Loss: 12.3404\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 44000/76666, Avg Loss: 12.3378\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 46000/76666, Avg Loss: 12.3320\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 48000/76666, Avg Loss: 12.3334\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 50000/76666, Avg Loss: 12.3322\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 52000/76666, Avg Loss: 12.3336\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 54000/76666, Avg Loss: 12.3304\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 56000/76666, Avg Loss: 12.3309\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 58000/76666, Avg Loss: 12.3379\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 60000/76666, Avg Loss: 12.3362\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 62000/76666, Avg Loss: 12.3349\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 64000/76666, Avg Loss: 12.3371\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 66000/76666, Avg Loss: 12.3366\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 68000/76666, Avg Loss: 12.3311\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 70000/76666, Avg Loss: 12.3316\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 72000/76666, Avg Loss: 12.3315\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 74000/76666, Avg Loss: 12.3279\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 76000/76666, Avg Loss: 12.3287\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "Epoch 19/20 Complete, Train MSE=12.3297, Test MSE=48.8757\n",
      "          R2-Score: 0.9184,           Accuracy: 0.0867\n",
      "\n",
      "Starting Epoch 20/20...\n",
      "  Batch 2000/76666, Avg Loss: 12.2999\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 4000/76666, Avg Loss: 12.3032\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 6000/76666, Avg Loss: 12.3111\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 8000/76666, Avg Loss: 12.3089\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 10000/76666, Avg Loss: 12.3159\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 12000/76666, Avg Loss: 12.3198\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 14000/76666, Avg Loss: 12.3232\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 16000/76666, Avg Loss: 12.3182\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 18000/76666, Avg Loss: 12.3108\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 20000/76666, Avg Loss: 12.3147\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 22000/76666, Avg Loss: 12.3107\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 24000/76666, Avg Loss: 12.3166\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 26000/76666, Avg Loss: 12.3177\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 28000/76666, Avg Loss: 12.3240\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 30000/76666, Avg Loss: 12.3255\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 32000/76666, Avg Loss: 12.3233\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 34000/76666, Avg Loss: 12.3197\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 36000/76666, Avg Loss: 12.3163\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 38000/76666, Avg Loss: 12.3190\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 40000/76666, Avg Loss: 12.3191\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 42000/76666, Avg Loss: 12.3152\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 44000/76666, Avg Loss: 12.3115\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 46000/76666, Avg Loss: 12.3075\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 48000/76666, Avg Loss: 12.3104\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 50000/76666, Avg Loss: 12.3109\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 52000/76666, Avg Loss: 12.3073\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 54000/76666, Avg Loss: 12.3034\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 56000/76666, Avg Loss: 12.3032\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 58000/76666, Avg Loss: 12.3021\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 60000/76666, Avg Loss: 12.2974\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 62000/76666, Avg Loss: 12.2963\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 64000/76666, Avg Loss: 12.2946\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 66000/76666, Avg Loss: 12.2936\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 68000/76666, Avg Loss: 12.2940\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 70000/76666, Avg Loss: 12.2920\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 72000/76666, Avg Loss: 12.2929\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 74000/76666, Avg Loss: 12.2959\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "  Batch 76000/76666, Avg Loss: 12.2981\n",
      "  Autosaved to trained_models/network_checkpoint_4.2.pth\n",
      "Epoch 20/20 Complete, Train MSE=12.2992, Test MSE=48.9969\n",
      "          R2-Score: 0.9182,           Accuracy: 0.0956\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from src.cnn import evaluate_test_loss, mse_torch\n",
    "\n",
    "# setzen des Optimizers sowie der Lernrate\n",
    "optimizer: torch.optim.Adam = torch.optim.Adam(\n",
    "    model.parameters(), \n",
    "    lr=LEARNING_RATE, \n",
    "    weight_decay=1e-5\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"min\", factor=0.5, patience=3\n",
    ")\n",
    "\n",
    "total_loss: list[Tensor] = []\n",
    "total_test_loss: list[Tensor] = []\n",
    "CHECKPOINT_PATH_SAVE: str = \"trained_models/network_checkpoint_4.2.pth\"\n",
    "autosave = 2000\n",
    "epochs: int = 20\n",
    "for e in range(epochs):\n",
    "    # model auf Trainingsmodus setzen\n",
    "    model.train()\n",
    "    error: float = 0.0\n",
    "    num_batches: int = 0\n",
    "    # previous_loss: float = 0.0\n",
    "    print(f\"Starting Epoch {e + 1}/{epochs}...\")\n",
    "\n",
    "    # durch die Batches des Trainingsloaders iterieren\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        # Daten auf das richtige Device verschieben und die Dimensionen anpassen\n",
    "        X_batch: Tensor = X_batch.to(DEVICE).unsqueeze(1)  # (B, 1, 10, 36)\n",
    "        y_batch: Tensor = y_batch.to(DEVICE).view(-1, 1)\n",
    "\n",
    "        # Gradienten zurücksetzen\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # Forward Pass\n",
    "        output: Tensor = model(X_batch)\n",
    "\n",
    "        # Überprüfen auf NaN-Werte im Output, da dies bereits einmal ein Fehler war\n",
    "        if torch.isnan(output).any():\n",
    "            print(f\"NaN detected in output at batch {num_batches}!\")\n",
    "            break\n",
    "\n",
    "        # Cost- / Loss-Funktion\n",
    "        loss: Tensor = mse_torch(y_batch, output)\n",
    "\n",
    "        # L2-Regularisierung\n",
    "        # l2_lambda: float = 1e-4\n",
    "        # l2_norm = 0.0\n",
    "        # for name, param in model.named_parameters():\n",
    "        #     if param.requires_grad and \"bias\" not in name:\n",
    "        #         l2_norm += torch.sum(param.pow(2))\n",
    "\n",
    "        # loss = loss + l2_lambda * l2_norm\n",
    "\n",
    "        # Backward Pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update der Gewichte\n",
    "        optimizer.step()\n",
    "\n",
    "        error += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "        # Model Autosave\n",
    "        if num_batches % autosave == 0:\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"epoch\": e,\n",
    "                    \"batch_idx\": num_batches,\n",
    "                    \"batch_size\": BATCH_SIZE,\n",
    "                    \"learning_rate\": LEARNING_RATE,\n",
    "                    \"model_state_dict\": model.state_dict(),\n",
    "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                },\n",
    "                CHECKPOINT_PATH_SAVE,\n",
    "            )\n",
    "            print(\n",
    "                f\"  Batch {num_batches}/{len(train_loader)}, Avg Loss: {error / num_batches:.4f}\"\n",
    "            )\n",
    "            print(f\"  Autosaved to {CHECKPOINT_PATH_SAVE}\")\n",
    "            total_loss.append(error / num_batches)\n",
    "            # test_loss, test_r2, test_accuracy = evaluate_test_loss(model, DEVICE, test_loader)\n",
    "            # total_test_loss.append(test_loss)\n",
    "\n",
    "        # Abbruchbedingung: starke Konvergenz der Verlustfunktion\n",
    "        # if abs(loss.item() - previous_loss) < 1e-3:\n",
    "        #     print(f\"  Loss change below threshold at batch {num_batches}, stopping early.\")\n",
    "        #     break\n",
    "        previous_loss = loss.item()\n",
    "\n",
    "    if loss:\n",
    "        total_loss.append(loss.item())\n",
    "    error /= max(num_batches, 1)\n",
    "    test_loss, test_r2, test_accuracy = evaluate_test_loss(\n",
    "        model, DEVICE, test_loader\n",
    "    )\n",
    "    total_test_loss.append(test_loss)\n",
    "    scheduler.step(test_loss)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {e + 1}/{epochs} Complete, Train MSE={error:.4f}, Test MSE={test_loss:.4f}\\n\\\n",
    "          R2-Score: {test_r2:.4f}, \\\n",
    "          Accuracy: {test_accuracy:.4f}\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6409b1d",
   "metadata": {},
   "source": [
    "## Plotting of a unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6bb600b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from src.cnn import load_model\n",
    "\n",
    "# model laden\n",
    "model = load_model(model, CHECKPOINT_PATH, DEVICE)\n",
    "# Unit zum Plotten auswählen\n",
    "test_unit_no = 7\n",
    "\n",
    "# Daten für die ausgewählte Unit extrahieren\n",
    "unit_data = df_testing_scaled[\n",
    "    df_testing_scaled[\"unit\"] == test_unit_no\n",
    "]\n",
    "unit_data_raw = df_testing[\n",
    "    df_testing[\"unit\"] == test_unit_no\n",
    "]  # unscaled, for real cycle numbers\n",
    "\n",
    "# Für Forward Pass\n",
    "X_test_unit: list[Tensor] = []\n",
    "y_test_unit: list[Tensor] = []\n",
    "# Für x-Achse im Plotten (echte Zyklenzahl, nicht skaliert)\n",
    "cycles_test_unit: list[int] = []\n",
    "\n",
    "# Erstellen von Testdaten im Window-Format für die ausgewählte Unit\n",
    "for i in range(0, len(unit_data) - WIN_LEN + 1):\n",
    "    temp = unit_data.iloc[i : i + WIN_LEN]  # skaliert, für die Vorhersage\n",
    "    temp_raw = unit_data_raw.iloc[\n",
    "        i : i + WIN_LEN\n",
    "    ]  # unskaliert, für die echten Zyklenzahlen\n",
    "    x_temp = temp.drop(columns=[\"unit\", \"RUL\"]).values\n",
    "    y_temp = temp[\"RUL\"].values[-1]\n",
    "    X_test_unit.append(x_temp)\n",
    "    y_test_unit.append(y_temp)\n",
    "    cycles_test_unit.append(temp_raw[\"cycle\"].values[-1])\n",
    "\n",
    "X_test_tensor: Tensor = torch.tensor(X_test_unit, dtype=torch.float32)\n",
    "dataset: TensorDataset = TensorDataset(X_test_tensor)\n",
    "loader: DataLoader = DataLoader(dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "predictions: list[Tensor] = []\n",
    "# forward pass, ohne Gradientenberechnung, da nur Predictions berechnet werden\n",
    "with torch.no_grad():\n",
    "    for (batch,) in loader:\n",
    "        batch = batch.to(DEVICE).unsqueeze(1)  # (B, 1, 10, 36)\n",
    "        batch_pred = model(batch)\n",
    "        predictions.append(batch_pred.cpu())\n",
    "\n",
    "# Predictions zusammenfügen, in numpy-Array umwandeln und flatten\n",
    "y_pred = torch.cat(predictions).numpy().flatten()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(cycles_test_unit, y_test_unit, label=\"True RUL\", marker=\"o\")\n",
    "plt.plot(cycles_test_unit, y_pred, label=\"Predicted RUL\", marker=\"x\")\n",
    "plt.title(f\"True vs Predicted RUL (Unit {test_unit_no})\")\n",
    "plt.xlabel(\"Cycle\")\n",
    "plt.ylabel(\"RUL\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd82fc09",
   "metadata": {},
   "source": [
    "## Interactive RUL Prediction Demo\n",
    "\n",
    "Wähle eine Triebwerkseinheit und ziehe den Slider, um festzulegen, wie viele Zyklen an Daten bekannt sein sollen. Die rote gestrichelte Linie markiert den aktuellen Zeitpunkt – alles links davon sind beobachtete Daten, und die Vorhersage zeigt, was das Modell zu diesem Zeitpunkt ausgeben würde.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d83b560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Pre-computing predictions for all units…\n",
      "==================================================\n",
      "[1/4] Processing unit 7… 392585 rows -> 392576 windows… done. RUL [0–89]\n",
      "[2/4] Processing unit 8… 800294 rows -> 800285 windows… done. RUL [0–88]\n",
      "[3/4] Processing unit 9… 351683 rows -> 351674 windows… done. RUL [0–79]\n",
      "[4/4] Processing unit 10… 1190670 rows -> 1190661 windows… done. RUL [0–81]\n",
      "==================================================\n",
      "All 4 units cached. Widgets are ready.\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72bc37d7a9754111bf18eede80e525fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Dropdown(description='Engine Unit:', layout=Layout(width='300px'), options=(7, 8…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from src.cnn import load_model\n",
    "from IPython.display import clear_output\n",
    "\n",
    "model = load_model(model, CHECKPOINT_PATH, DEVICE)\n",
    "\n",
    "# Data Loading\n",
    "print(\"=\" * 50)\n",
    "print(\"Pre-computing predictions for all units…\")\n",
    "print(\"=\" * 50, flush=True)\n",
    "\n",
    "unit_cache: dict[int, dict[str, np.ndarray]] = {}\n",
    "available_units: list[int] = sorted(\n",
    "    df_testing[\"unit\"].unique().astype(int).tolist()\n",
    ")\n",
    "total_units: int = len(available_units)\n",
    "\n",
    "for idx_u, unit_no in enumerate(available_units, start=1):\n",
    "    print(\n",
    "        f\"[{idx_u}/{total_units}] Processing unit {unit_no}…\",\n",
    "        end=\" \",\n",
    "        flush=True,\n",
    "    )\n",
    "\n",
    "    unit_scaled: pd.DataFrame = df_testing_scaled[\n",
    "        df_testing_scaled[\"unit\"] == unit_no\n",
    "    ].reset_index(drop=True)\n",
    "    unit_raw: pd.DataFrame = df_testing[\n",
    "        df_testing[\"unit\"] == unit_no\n",
    "    ].reset_index(drop=True)\n",
    "\n",
    "    if len(unit_scaled) < WIN_LEN:\n",
    "        unit_cache[unit_no] = {\"cycles\": [], \"y_true\": [], \"y_pred\": []}\n",
    "        print(\"skipped (not enough cycles).\", flush=True)\n",
    "        continue\n",
    "\n",
    "    n: int = len(unit_scaled)\n",
    "    feat_cols: list[str] = [\n",
    "        c for c in unit_scaled.columns if c not in (\"unit\", \"RUL\")\n",
    "    ]\n",
    "    X_arr: np.ndarray = unit_scaled[feat_cols].values\n",
    "\n",
    "    print(f\"{n} rows -> {n - WIN_LEN + 1} windows…\", end=\" \", flush=True)\n",
    "\n",
    "    X_windows: np.ndarray = np.stack(\n",
    "        [X_arr[i : i + WIN_LEN] for i in range(n - WIN_LEN + 1)]\n",
    "    )\n",
    "\n",
    "    y_true_list, cycles_list = [], []\n",
    "    for i in range(n - WIN_LEN + 1):\n",
    "        y_true_list.append(unit_scaled[\"RUL\"].values[i + WIN_LEN - 1])\n",
    "        cycles_list.append(unit_raw[\"cycle\"].values[i + WIN_LEN - 1])\n",
    "    y_true_arr: np.ndarray = np.array(y_true_list)\n",
    "    cycles_arr = np.array(cycles_list)\n",
    "\n",
    "    X_tensor: Tensor = torch.tensor(X_windows, dtype=torch.float32)\n",
    "    loader: DataLoader = DataLoader(\n",
    "        TensorDataset(X_tensor), batch_size=256, shuffle=False\n",
    "    )\n",
    "\n",
    "    preds: list[Tensor] = []\n",
    "    with torch.no_grad():\n",
    "        for (batch,) in loader:\n",
    "            preds.append(model(batch.to(DEVICE).unsqueeze(1)).cpu())\n",
    "\n",
    "    y_pred_arr: np.ndarray = torch.cat(preds).numpy().flatten()\n",
    "\n",
    "    unit_cache[unit_no]: dict[str, np.ndarray] = {\n",
    "        \"cycles\": cycles_arr,\n",
    "        \"y_true\": y_true_arr,\n",
    "        \"y_pred\": y_pred_arr,\n",
    "    }\n",
    "\n",
    "    rul_range = f\"RUL [{y_true_arr.min():.0f}–{y_true_arr.max():.0f}]\"\n",
    "    print(f\"done. {rul_range}\", flush=True)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(f\"All {total_units} units cached. Widgets are ready.\")\n",
    "print(\"=\" * 50, flush=True)\n",
    "\n",
    "\n",
    "# Plotting\n",
    "def plot_agent(unit_no: int, cycle_cutoff: int) -> None:\n",
    "    plt.close(\"all\")\n",
    "\n",
    "    cache: dict[str, np.ndarray] = unit_cache[unit_no]\n",
    "    cycles: np.ndarray = np.array(cache[\"cycles\"])\n",
    "    y_true: np.ndarray = np.array(cache[\"y_true\"])\n",
    "    y_pred: np.ndarray = np.array(cache[\"y_pred\"])\n",
    "\n",
    "    if len(cycles) == 0:\n",
    "        print(f\"Unit {unit_no}: not enough cycles.\")\n",
    "        return\n",
    "\n",
    "    # Observed: all windows whose last cycle <= cutoff\n",
    "    mask_obs: np.ndarray = cycles <= cycle_cutoff\n",
    "\n",
    "    if not mask_obs.any():\n",
    "        print(f\"Unit {unit_no}: no observed windows at cycle {cycle_cutoff}.\")\n",
    "        return\n",
    "\n",
    "    obs_cycles: np.ndarray = cycles[mask_obs]\n",
    "    obs_true: np.ndarray = y_true[mask_obs]\n",
    "    obs_pred: np.ndarray = y_pred[mask_obs]\n",
    "\n",
    "    # Forecast: windows AFTER cutoff — model already saw these during caching\n",
    "    mask_fore: np.ndarray = cycles > cycle_cutoff\n",
    "    fore_cycles: np.ndarray = cycles[mask_fore]\n",
    "    fore_pred: np.ndarray = y_pred[mask_fore]\n",
    "\n",
    "    # RUL at cutoff = last predicted value in observed window\n",
    "    rul_at_cutoff: float = obs_pred[-1]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(13, 5))\n",
    "\n",
    "    # True RUL (solid blue)\n",
    "    ax.plot(\n",
    "        obs_cycles,\n",
    "        obs_true,\n",
    "        color=\"steelblue\",\n",
    "        linewidth=1.8,\n",
    "        marker=\"o\",\n",
    "        markersize=2.5,\n",
    "        label=\"True RUL (observed)\",\n",
    "    )\n",
    "\n",
    "    # Predicted RUL in observed range (solid orange)\n",
    "    ax.plot(\n",
    "        obs_cycles,\n",
    "        obs_pred,\n",
    "        color=\"darkorange\",\n",
    "        linewidth=1.5,\n",
    "        marker=\"x\",\n",
    "        markersize=2.5,\n",
    "        label=\"Predicted RUL (observed)\",\n",
    "    )\n",
    "\n",
    "    # Forecast: connect smoothly from last observed pred into future (dashed)\n",
    "    if mask_fore.any():\n",
    "        # Prepend the last observed point so the dashed line starts without a gap\n",
    "        fore_cycles_full: np.ndarray = np.concatenate(\n",
    "            [[obs_cycles[-1]], fore_cycles]\n",
    "        )\n",
    "        fore_pred_full: np.ndarray = np.concatenate(\n",
    "            [[rul_at_cutoff], fore_pred]\n",
    "        )\n",
    "        ax.plot(\n",
    "            fore_cycles_full,\n",
    "            fore_pred_full,\n",
    "            color=\"darkorange\",\n",
    "            linewidth=2.0,\n",
    "            linestyle=\"--\",\n",
    "            marker=\"\",\n",
    "            label=\"Predicted RUL (forecast)\",\n",
    "        )\n",
    "\n",
    "    # Cutoff line\n",
    "    ax.axvline(\n",
    "        cycle_cutoff,\n",
    "        color=\"crimson\",\n",
    "        linestyle=\"--\",\n",
    "        linewidth=2.0,\n",
    "        label=f\"Now — cycle {cycle_cutoff}\",\n",
    "    )\n",
    "\n",
    "    # RUL annotation\n",
    "    ax.annotate(\n",
    "        f\"  RUL ≈ {rul_at_cutoff:.0f} cycles\",\n",
    "        xy=(cycle_cutoff, rul_at_cutoff),\n",
    "        xytext=(\n",
    "            cycle_cutoff + (cycles.max() - cycles.min()) * 0.03,\n",
    "            rul_at_cutoff,\n",
    "        ),\n",
    "        fontsize=11,\n",
    "        color=\"crimson\",\n",
    "        arrowprops=dict(arrowstyle=\"->\", color=\"crimson\", lw=1.2),\n",
    "    )\n",
    "\n",
    "    ax.set_title(\n",
    "        f\"RUL Prediction  |  Unit {unit_no}  |  Observed up to cycle {cycle_cutoff}\",\n",
    "        fontsize=13,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "    ax.set_xlabel(\"Cycle\", fontsize=11)\n",
    "    ax.set_ylabel(\"RUL (cycles remaining)\", fontsize=11)\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.grid(True, alpha=0.35)\n",
    "    plt.tight_layout()\n",
    "    display(fig)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "# Widgets\n",
    "max_cycle: int = int(df_testing[\"cycle\"].max())\n",
    "\n",
    "unit_widget: widgets.Dropdown = widgets.Dropdown(\n",
    "    options=available_units,\n",
    "    value=available_units[0],\n",
    "    description=\"Engine Unit:\",\n",
    "    style={\"description_width\": \"initial\"},\n",
    "    layout=widgets.Layout(width=\"300px\"),\n",
    ")\n",
    "\n",
    "cycle_widget: widgets.IntSlider = widgets.IntSlider(\n",
    "    min=WIN_LEN,\n",
    "    max=max_cycle,\n",
    "    step=1,\n",
    "    value=50,\n",
    "    description=\"Current cycle:\",\n",
    "    style={\"description_width\": \"initial\"},\n",
    "    layout=widgets.Layout(width=\"600px\"),\n",
    "    continuous_update=False,\n",
    ")\n",
    "\n",
    "out: widgets.interactive = widgets.interactive_output(\n",
    "    plot_agent, {\"unit_no\": unit_widget, \"cycle_cutoff\": cycle_widget}\n",
    ")\n",
    "display(widgets.VBox([widgets.HBox([unit_widget]), cycle_widget, out]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmapps (3.14.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
